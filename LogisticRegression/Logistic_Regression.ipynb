{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Exoplanet Vetter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd # to read CSV\n",
    "from sklearn import model_selection, preprocessing # for multivariable linear regression\n",
    "import sklearn.linear_model as lm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import sklearn.metrics as skm\n",
    "import h5py\n",
    "import gc\n",
    "import pickle\n",
    "import time\n",
    "for obj in gc.get_objects():   # Browse through ALL objects\n",
    "    if isinstance(obj, h5py.File):   # Just HDF5 files\n",
    "        try:\n",
    "            obj.close()\n",
    "        except:\n",
    "            pass # Was already closed\n",
    "\n",
    "sectors = [\"sector-{}\".format(i) for i in [1,2,3,4,5,6,7,8,9,10,11,12,13,14,17,18,19,20,21]]\n",
    "\n",
    "# Manually point to TICS folder which contains labels.tsv\n",
    "TICS_dropbox = \"/Users/hblim/Dropbox (MIT)/qlp-data/TICS\"\n",
    "\n",
    "# Manually point to TICS folder which has preprocessed lightcurves\n",
    "TICS_drive   = \"/Volumes/halston_lim/School_Documents/SP_2019-2020/6.862/TICS/updatedH5\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 1: Import Old Data and Count existing files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "  # Features:\n",
    "  #   [0] astronet score\n",
    "  #   [1] depth best ap - 1\n",
    "  #   [2] depth best ap (global)\n",
    "  #   [3] depth best ap + 1 or best ap\n",
    "  #   [4] error best ap - 1\n",
    "  #   [5] error best ap (global)\n",
    "  #   [6] error best ap + 1\n",
    "  # \n",
    "  #   [7] depth best ap - 1\n",
    "  #   [8] depth best ap (local)\n",
    "  #   [9] depth best ap + 1 or best ap\n",
    "  #   [10] error best ap - 1\n",
    "  #   [11] error best ap (local)\n",
    "  #   [12] error best ap + 1\n",
    "  # \n",
    "  #   [13] depth even\n",
    "  #   [14] depth odd\n",
    "  #   [15] error even\n",
    "  #   [16] error odd\n",
    "\n",
    "  #   [17] depth secondary\n",
    "  #   [18] error seconday\n",
    "\n",
    "  #   [19] label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "145415 lightcurve files\n"
     ]
    }
   ],
   "source": [
    "# get all files that have lightcurves\n",
    "filelist = []\n",
    "for i,sector in enumerate(sectors):\n",
    "    filelist += os.listdir(os.path.join(TICS_drive,sector,\"preprocessed\"))\n",
    "filelist = list(set(filelist))\n",
    "print(len(filelist),\"lightcurve files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total files:  134628\n",
      "total planets:  800\n",
      "total missing:  893\n"
     ]
    }
   ],
   "source": [
    "labels_tsv = np.genfromtxt(os.path.join(TICS_dropbox,\"labels.tsv\"),skip_header = 3,usecols=(0),dtype='i8')\n",
    "\n",
    "# get files from \n",
    "filelist = open(os.path.join(TICS_drive,\"filelist.txt\"),'r').readlines()\n",
    "filelist = set([i.strip() for i in filelist])\n",
    "\n",
    "labels_str = set([str(labels_tsv[i]) + '.h5' for i in range(len(labels_tsv))])\n",
    "print(\"total files: \", len(filelist))\n",
    "print(\"total planets: \", len(filelist.intersection(labels_str)))\n",
    "print(\"total missing: \", len(labels_str - filelist.intersection(labels_str)))\n",
    "\n",
    "missing = list(labels_str - filelist.intersection(labels_str))\n",
    "fout = open(os.path.join(TICS_drive,\"missing.txt\"),'w')\n",
    "for m in missing:\n",
    "    fout.write(m + '\\n')\n",
    "fout.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "# Global folder paths\n",
    "datapath  = \"/Volumes/halston_lim/School_Documents/SP_2019-2020/6.862/TICS/\"\n",
    "\n",
    "# Manually include which sectors names to scan\n",
    "sectors = [\"sector-{}\".format(i) for i in \\\n",
    "           [1,2,3,4,5,6,7,8,9,10,11,12,13,14,17,18,19,20,21]]\n",
    "\n",
    "data = np.zeros((0,20))\n",
    "for i in range(len(sectors)):\n",
    "    datatemp = np.load(os.path.join(datapath,sectors[i],'lcFeatures.npy'))\n",
    "    data = np.append(data,datatemp,axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "hide_input": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-5dd017e5848d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mn_planet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mn_nonplanet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mn_planet\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_nonplanet\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_planet\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mn_nonplanet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mfdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m13\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m14\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m17\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m19\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "n_planet = np.sum(data[:,-1] == 1)\n",
    "n_nonplanet = np.sum(data[:,-1] == 0)\n",
    "n_planet,n_nonplanet,n_planet / n_nonplanet\n",
    "\n",
    "fdata = data[:,[0,1,2,3,7,8,9,13,14,17,19]]\n",
    "X = fdata[:,0:-1]\n",
    "y = fdata[:,-1] * 2 - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Options 2: Import New Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readPickle(filepath):\n",
    "    assert os.path.exists(filepath)\n",
    "\n",
    "    with open(filepath,'rb') as f: data = pickle.load(f)\n",
    "\n",
    "    nfiles = len(data)\n",
    "\n",
    "    #views\n",
    "    localviews = np.zeros((nfiles,3,61))\n",
    "    globalviews = np.zeros((nfiles,3,201))\n",
    "\n",
    "    std_depths     = np.zeros((nfiles,2))\n",
    "    true_depths    = np.zeros((nfiles,6))\n",
    "    depth_errors   = np.zeros((nfiles,6))\n",
    "    astronet_score = np.zeros((nfiles,1))\n",
    "    stellar_radii  = np.zeros((nfiles,1))\n",
    "    magnitudes     = np.zeros((nfiles, 1))\n",
    "\n",
    "    other_stuff    = np.zeros((nfiles,3))\n",
    "\n",
    "\n",
    "    # Labels\n",
    "    labels = np.zeros((nfiles,))\n",
    "\n",
    "    for i,file in enumerate(data):\n",
    "        \n",
    "        stdcut = (np.std(file['LocalDepths']) < 1) and (np.std(file['GlobalDepths']) < 1)\n",
    "        non_zero_depths = ((np.sum(file['LocalDepths'] == 0) + np.sum(file['GlobalDepths'] == 0)) == 0)\n",
    "        no_nans = np.sum(np.isnan(file['LocalView'])) + np.sum(np.isnan(file['GlobalView'])) == 0\n",
    "\n",
    "        if no_nans and non_zero_depths and stdcut:\n",
    "            localviews[i] = file['LocalView']\n",
    "            globalviews[i] = file['GlobalView']\n",
    "            labels[i] = file['label']\n",
    "\n",
    "            astronet_score[i] = np.array([file['AstroNetScore']])\n",
    "            std_depths[i]     = np.array([np.std(file['LocalDepths']),np.std(file['GlobalDepths'])])\n",
    "            true_depths[i]    = np.hstack([file['LocalDepths'],file['GlobalDepths']])\n",
    "            depth_errors[i]    = np.hstack([file['LocalDepthErrors'],file['GlobalDepthErrors']])\n",
    "            stellar_radii[i]  = np.array([file['StellarParams']['rad']])\n",
    "            magnitudes[i]     = np.array(file['StellarParams']['tmag'])\n",
    "\n",
    "            other_stuff[i]    = np.append(file['EvenOddDepths'],file['SecondaryDepth'])\n",
    "\n",
    "\n",
    "    return labels, localviews, globalviews, std_depths, true_depths, depth_errors, astronet_score, stellar_radii, magnitudes, other_stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadSortedData(pickleDir, sectors, verbose=True):\n",
    "    std_depths = np.zeros((0,2))\n",
    "    true_depths = np.zeros((0,6))\n",
    "    depth_errors = np.zeros((0,6))\n",
    "    astronets = np.zeros((0,1))\n",
    "    stellar_radii = np.zeros((0,1))\n",
    "    all_mag    = np.zeros((0,1))\n",
    "\n",
    "    all_localviews = np.zeros((0,3,61))\n",
    "    all_globalviews = np.zeros((0,3,201))\n",
    "    all_labels = np.zeros((0,))\n",
    "\n",
    "    all_other = np.zeros((0,3))\n",
    "\n",
    "    # append with data from each sector\n",
    "    for sector in tqdm(sectors):\n",
    "        \n",
    "        filepath = pickleDir + sector + '.pickle'\n",
    "        labels, localviews, globalviews, std_depth, true_depth, depth_error, astronet_score, stellar_rad, magnitude, other_stuff = readPickle(filepath)\n",
    "\n",
    "        std_depths    = np.append(std_depths, std_depth,axis=0)\n",
    "        true_depths   = np.append(true_depths,true_depth,axis=0)\n",
    "        depth_errors  = np.append(depth_errors, depth_error, axis=0)\n",
    "        astronets     = np.append(astronets, astronet_score, axis = 0)\n",
    "        stellar_radii = np.append(stellar_radii, stellar_rad , axis = 0)\n",
    "        all_mag       = np.append(all_mag, magnitude, axis=0)\n",
    "\n",
    "        all_localviews = np.append(all_localviews, localviews,axis=0)\n",
    "        all_globalviews = np.append(all_globalviews, globalviews,axis=0)\n",
    "        all_labels = np.append(all_labels,labels)\n",
    "\n",
    "        all_other = np.append(all_other, other_stuff, axis=0)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"X_global.shape = \", all_globalviews.shape)\n",
    "        print(\"X_local.shape  = \", all_localviews.shape)\n",
    "        print(\"X_depths.shape = \", true_depths.shape)\n",
    "        print(\"y.shape        = \", all_labels.shape)\n",
    "\n",
    "    return all_labels, all_localviews, all_globalviews, true_depths, depth_errors, std_depths, astronets, stellar_radii, all_mag, all_other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21/21 [00:54<00:00,  2.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_global.shape =  (248392, 3, 201)\n",
      "X_local.shape  =  (248392, 3, 61)\n",
      "X_depths.shape =  (248392, 6)\n",
      "y.shape        =  (248392,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def addToDataMatrix(X, addon):\n",
    "    n = np.shape(X)[0]\n",
    "    ncols = np.prod(np.shape(addon)[1:])\n",
    "    X = np.append(X, addon.reshape(n,ncols), axis=1)\n",
    "    return X\n",
    "\n",
    "def genDataMatrix(y, *features):\n",
    "    n = len(y)\n",
    "    X = np.zeros((n,0))\n",
    "    for feat in features:\n",
    "        X = addToDataMatrix(X, feat)\n",
    "    return X, y\n",
    "\n",
    "pickleDir = \"/Users/hblim/Dropbox (MIT)/qlp-data/2020_05_updatedLC/\"\n",
    "\n",
    "sectors = [f'sector-{i}' for i in range(1,22)]\n",
    "labels, localviews, globalviews, depths, depth_err, std_depths, astronets, srad, tmag , other= loadSortedData(pickleDir, sectors)\n",
    "eod = other[:,:2]\n",
    "eostd = np.std(eod,1,keepdims=True)\n",
    "\n",
    "\n",
    "X, y = genDataMatrix(labels, std_depths, astronets, tmag, depths, other, eostd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/hblim/Dropbox (MIT)/qlp-data/2020_05_updatedLC/sector-1.pickle\n",
      "/Users/hblim/Dropbox (MIT)/qlp-data/2020_05_updatedLC/sector-2.pickle\n",
      "/Users/hblim/Dropbox (MIT)/qlp-data/2020_05_updatedLC/sector-3.pickle\n",
      "/Users/hblim/Dropbox (MIT)/qlp-data/2020_05_updatedLC/sector-4.pickle\n",
      "/Users/hblim/Dropbox (MIT)/qlp-data/2020_05_updatedLC/sector-5.pickle\n",
      "/Users/hblim/Dropbox (MIT)/qlp-data/2020_05_updatedLC/sector-6.pickle\n",
      "/Users/hblim/Dropbox (MIT)/qlp-data/2020_05_updatedLC/sector-7.pickle\n",
      "/Users/hblim/Dropbox (MIT)/qlp-data/2020_05_updatedLC/sector-8.pickle\n",
      "/Users/hblim/Dropbox (MIT)/qlp-data/2020_05_updatedLC/sector-9.pickle\n",
      "/Users/hblim/Dropbox (MIT)/qlp-data/2020_05_updatedLC/sector-10.pickle\n",
      "/Users/hblim/Dropbox (MIT)/qlp-data/2020_05_updatedLC/sector-11.pickle\n",
      "/Users/hblim/Dropbox (MIT)/qlp-data/2020_05_updatedLC/sector-12.pickle\n",
      "/Users/hblim/Dropbox (MIT)/qlp-data/2020_05_updatedLC/sector-13.pickle\n",
      "/Users/hblim/Dropbox (MIT)/qlp-data/2020_05_updatedLC/sector-14.pickle\n",
      "/Users/hblim/Dropbox (MIT)/qlp-data/2020_05_updatedLC/sector-15.pickle\n",
      "/Users/hblim/Dropbox (MIT)/qlp-data/2020_05_updatedLC/sector-16.pickle\n",
      "/Users/hblim/Dropbox (MIT)/qlp-data/2020_05_updatedLC/sector-17.pickle\n",
      "/Users/hblim/Dropbox (MIT)/qlp-data/2020_05_updatedLC/sector-18.pickle\n",
      "/Users/hblim/Dropbox (MIT)/qlp-data/2020_05_updatedLC/sector-19.pickle\n",
      "/Users/hblim/Dropbox (MIT)/qlp-data/2020_05_updatedLC/sector-20.pickle\n",
      "/Users/hblim/Dropbox (MIT)/qlp-data/2020_05_updatedLC/sector-21.pickle\n",
      "X_global.shape =  (248392, 3, 201)\n",
      "X_local.shape  =  (248392, 3, 61)\n",
      "X_depths.shape =  (248392, 6)\n",
      "y.shape        =  (248392,)\n"
     ]
    }
   ],
   "source": [
    "verbose = 1\n",
    "pickle_dir = \"/Users/hblim/Dropbox (MIT)/qlp-data/2020_05_updatedLC/\"\n",
    "\n",
    "std_depths = np.zeros((0,2))\n",
    "true_depths = np.zeros((0,6))\n",
    "astronets = np.zeros((0,1))\n",
    "stellar_radii = np.zeros((0,1))\n",
    "\n",
    "all_localviews = np.zeros((0,3,61))\n",
    "all_globalviews = np.zeros((0,3,201))\n",
    "all_labels = np.zeros((0,),dtype=int)\n",
    "\n",
    "# append with data from each sector\n",
    "for sector in range(1,22):\n",
    "    \n",
    "    filepath = pickle_dir + 'sector-{}.pickle'.format(sector)\n",
    "    print(filepath)\n",
    "    localviews, globalviews, std_depth, true_depth, astronet_score, stellar_rad, labels = read_pickle(filepath)\n",
    "    \n",
    "    std_depths = np.append(std_depths, std_depth,axis=0)\n",
    "    true_depths = np.append(true_depths,true_depth,axis=0)\n",
    "    astronets = np.append(astronets, astronet_score, axis = 0)\n",
    "    stellar_radii = np.append(stellar_radii, stellar_rad , axis = 0)\n",
    "    \n",
    "    \n",
    "    all_localviews = np.append(all_localviews, localviews,axis=0)\n",
    "    all_globalviews = np.append(all_globalviews, globalviews,axis=0)\n",
    "    all_labels = np.append(all_labels,labels)\n",
    "\n",
    "if verbose:\n",
    "    print(\"X_global.shape = \", all_globalviews.shape)\n",
    "    print(\"X_local.shape  = \", all_localviews.shape)\n",
    "    print(\"X_depths.shape = \", true_depths.shape)\n",
    "    print(\"y.shape        = \", all_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('int64')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verbose = 1\n",
    "# ntotal = len(all_labels)\n",
    "# X = np.zeros((ntotal,0))\n",
    "# # X = np.append(X, all_localviews.reshape((ntotal,3*61)),axis=1)\n",
    "# # X = np.append(X, all_globalviews.reshape((ntotal,3*201)),axis=1)\n",
    "# X = np.append(X, std_depths.reshape((ntotal,2)),axis=1)\n",
    "# X = np.append(X, astronets.reshape((ntotal,1)),axis=1)\n",
    "# # X = np.append(X, true_depths.reshape((ntotal,6)),axis=1)\n",
    "# X = np.append(X, all_labels.reshape((ntotal,1)),axis=1)\n",
    "\n",
    "# # downsample majority class\n",
    "# PCs = all_labels == 1\n",
    "# nonPCs = all_labels == 0\n",
    "# X_PC = X[PCs]\n",
    "# X_nPC = X[nonPCs]\n",
    "\n",
    "# np.random.shuffle(X_nPC)\n",
    "# # X_nPC = X_nPC[0:int(np.sum(nonPCs) / 10)]\n",
    "# # X_nPC = X_nPC[0:int(np.sum(PCs)) ]\n",
    "\n",
    "# Xfil = np.vstack([X_PC,X_nPC])\n",
    "# np.random.shuffle(Xfil)\n",
    "\n",
    "\n",
    "# X = Xfil[:,:-1]\n",
    "# y = Xfil[:,-1].astype(int)\n",
    "# # if verbose: print(\"X.shape = \", X.shape)\n",
    "y = y.astype('int64')\n",
    "y.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use all data columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data and labels $X$ and $y$ should be defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xnorm = X[:,0:4]\n",
    "\n",
    "kfold_score = [] # RMSE per case\n",
    "kfold_predict = [] # predict class\n",
    "\n",
    "n_cases = (X.shape)[0]\n",
    "nfolds = 5\n",
    "nthreshs = 20\n",
    "kfold_predict = []\n",
    "kfold_classifies = np.zeros((nfolds,5),dtype='i8')\n",
    "kfold_probabilities = np.zeros((nfolds,5,nthreshs),dtype='i8')\n",
    "\n",
    "precision_recall = []\n",
    "\n",
    "kfold = model_selection.StratifiedKFold(n_splits = nfolds,shuffle=True)\n",
    "i = 0\n",
    "t0 = time.time()\n",
    "    # scale training data\n",
    "sc = preprocessing.StandardScaler().fit(X)\n",
    "Xnorm = sc.transform(X)\n",
    "Xp = Xnorm[:,0:4]\n",
    "\n",
    "print(Xp.shape)\n",
    "\n",
    "for train_index, test_index in kfold.split(Xp,y):\n",
    "\n",
    "    t0 = time.time()\n",
    "    \n",
    "    # split into test, training\n",
    "    X_train, X_test = Xp[train_index], Xp[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "\n",
    "    \n",
    "    # regress\n",
    "    reg = lm.LogisticRegression( \\\n",
    "                                penalty='l2',\n",
    "                                fit_intercept=True,\n",
    "                                C = 1.0,\n",
    "                                random_state=420,\n",
    "                                solver='sag',\n",
    "                                tol = 1e-3,\n",
    "                                class_weight = 'balanced',\n",
    "                                max_iter = 1000,\n",
    "                                multi_class='ovr',\n",
    "                                warm_start=True\n",
    "                               )\n",
    "    reg.fit(X_train,y_train)\n",
    "\n",
    "    # scale test data and predict\n",
    "#     X_test_sc = scaler.transform(X_test)\n",
    "    prediction = reg.predict(X_test)\n",
    "    probabilities = reg.predict_proba(X_test)\n",
    "    kfold_predict.append(prediction)\n",
    "    \n",
    "    precision_recall.append(skm.precision_recall_curve(np.array(y_test),probabilities[:,1],pos_label=1))\n",
    "\n",
    "    # evaluate fold\n",
    "    yt = np.array(y_test)\n",
    "    right = (yt - prediction == 0)\n",
    "    planet = (yt == 1)\n",
    "    \n",
    "    n_total = len(yt)\n",
    "    n_true = np.sum(right)\n",
    "    n_true_planet = np.sum(np.logical_and(right,planet))\n",
    "    n_falsepos = np.sum(np.logical_and(~right,~planet))\n",
    "    n_falseneg = np.sum(np.logical_and(~right,planet))\n",
    "\n",
    "    kfold_classifies[i][0] = n_total\n",
    "    kfold_classifies[i][1] = np.sum(planet)\n",
    "    kfold_classifies[i][2] = n_true_planet\n",
    "    kfold_classifies[i][3] = n_falsepos\n",
    "    kfold_classifies[i][4] = n_falseneg\n",
    "    \n",
    "    \n",
    "    print(\"Fold #{}: {:.4f}\".format(i,time.time() -t0))\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (6,3),dpi = 150)\n",
    "\n",
    "xp = np.linspace(0.01,.99,1000)\n",
    "yp = np.zeros((nfolds,len(xp)))\n",
    "\n",
    "colors = ['b','g','r','c','m']\n",
    "\n",
    "for i in range(nfolds):\n",
    "    noskill = kfold_classifies[i,1] / kfold_classifies[i,0] \n",
    "    plt.plot(precision_recall[i][1][:-1],precision_recall[i][0][:-1],color=colors[i],alpha=0.2,label=\"fold {}\".format(i))\n",
    "    plt.plot([0,1],[noskill,noskill],color=colors[i],linestyle='--',alpha=0.2)\n",
    "    yp[i] = np.interp(xp,precision_recall[i][1][-1:0:-1],precision_recall[i][0][-1:0:-1])\n",
    "    print(yp[i].shape)\n",
    "\n",
    "# average\n",
    "plt.plot(xp,np.mean(yp,axis=0),'k-',label='k-fold average')\n",
    "\n",
    "plt.legend(frameon=False,ncol=nfolds,bbox_to_anchor=(0,1),loc='lower left')\n",
    "plt.ylabel('Precision')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylim([0,.4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
